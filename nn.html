<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Networks</title>
    <link rel="stylesheet" href="style.css" type="text/css"></head>

<body>
    <header>

        <h1>Machine Learning</h1>
        <h2>Neural Networks</h2>
        <nav role="navigation">
            <ul>
                <li><a href="intro.html" title="Introduction to Machine Learning">Introduction to Machine Learning</a></li>
                <li><a href="linreg.html" title="Linear Regression">Linear Regression</a></li>
                <li><a href="logreg.html" title="Logistic Regression">Logistic Regression</a></li>
                <li><a href="nn.html" title="Neural Networks (Supervised)">Neural Networks (Supervised)</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <article>


            <img src="assets/NeuralNetworkInference.gif" alt="" class="flowRight">

            <p>
                Neural networks are what's hot on the streets right now. They were actually proposed a long time ago but while the literature was getting
                there, we did not yet have the computational power to fully take advantage of their architecture. Another issue is that the expectation that
                everyone had for them was far too high. People were really excited about the idea of Artificial General Intelligence; that is, a try A.I. that
                could function at and beyond human capacity. This has not been seen yet but whether it takes 20, 40, or 100 years we'll get there; that is, if 
                we don't blow ourselves up first.
            </p>

            <p>
                The gif on right is an outstanding visualization created by Grant Sanderson, the creator of <a href="https://www.youtube.com/3Blue1Brown">3Blue1Brown</a> 
                youtube channel that uses programmatically animated visualizations to explain mathematical concepts; mostly in the area of applied mathematics. 
                It depicts how a neural network takes in an input, propagates the data through its neurons, and picks one of the preset
                outputs that represent a discrete number of possibilities. In this case its taking a 28x28 pixel image of a number, flattening it to 784 values, 
                28*28=784, and picks the number that its trained to recognize. 
            </p>
            
            <img src="assets/NeuralNetworkTraining.gif" alt="Neural Network Training" class="flowLeft">
            
            <p>
                A neural network is a mathematical entity that can approximate any function but some functions are easier to approximate then others. There is a 
                relationship between the complexity of the problem and the structure of the parameters of the network. If the function you are modeling is polynomial,
                you would want as many weights as there are degrees in that polynomial, to capture all of the variance in the data generated by it.
            </p>
            
            <p>
                With neural networks, it works in the same way. With extremely complex functions, we need more parameters to represent it. 
            </p>

            <p>
                With a single feed-forward layer, you can model any function, but it can grow exponentially large. By giving the model multiple layers, we force it to
                learn combinations of different activations to make decisions, and these combinations of activations will generalize better because they haven't memorized
                the data with an exact sequence of values, but with features that are representations of some pattern in the data. 
            </p>

            <img src="assets/NeuralNetworkError.gif" alt="Neural Network Error" class="flowRight">

            <p>
                If you have too many weights, it can use one trajectory of weights for each possible data point it sees, and those weights may not <strong>need</strong> to
                be reused to learn other data points, and so it is not learning to abstract its understanding of the data which is a sort of memorization of data. 
            </p>

            <p>
                So when implementing a neural network you have to try to match the complexity of the function with the structure of the network, otherwise you might overfit or underfit
                and get garbage outputs.
            </p>

            <p>More to come soon!</p>

        </article>
    </main>

    <footer>
        &copy; 2020 Jorge Martinez
    </footer>
</body>

</html>